{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import wrds\n",
    "import pathlib\n",
    "import yfinance as yf\n",
    "from pathlib import Path  \n",
    "\n",
    "# Parent folder\n",
    "pf = pathlib.Path().resolve() # Points to parent folder containing notebook and data, eg: 'C:/Users/Carla/Dropbox/Uni/10. Semester/Dynamic Programming/Term paper'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Process:\n",
    "1. Download option chain data for from WRDS SECID by year \n",
    "2. Combine with data from other sources (yfinance, FRED)\n",
    "3. Prepare data for estimation applying filters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to wrds\n",
    "usr = '**'\n",
    "db = wrds.Connection(wrds_username=usr)\n",
    "\n",
    "# Creating password file - only for first time setup\n",
    "db.create_pgpass_file() # PW = ******\n",
    "#sorted(db.list_libraries()) # List of all available libraries\n",
    "#db.list_tables(library=\"optionm\") # List of all tables in optionm library\n",
    "#db.describe_table(library=\"optionm\", table=\"opprcd1996\") # list of variables in table opprcd1996 - Note there is one table for each year e.g: oprcdXXXX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Option data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify which securityid (index) to get data for\n",
    "\n",
    "secids = [108105, 102480, 102456, 102434, 143439, 101310, 12182]\n",
    "names = ['SPX','NDX','DJX', 'RUT', 'TSLA', 'AMZN', 'GOOGL']\n",
    "\n",
    "# Specify years\n",
    "years = np.arange(1996, 2022, 1)\n",
    "\n",
    "for i, secid in enumerate(secids):\n",
    "    # Set up empty table to gather data for each year\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    # Get data for each year and append:\n",
    "    for year in years:\n",
    "        # Set up SQL expression to pass on\n",
    "        sql = f\"select secid, date, exdate, cp_flag, strike_price, best_bid, best_offer, volume, open_interest, impl_volatility, delta from optionm.opprcd{year} where secid = '{secid}'\"\n",
    "\n",
    "        # Get data \n",
    "        data_1year = db.raw_sql(sql)\n",
    "\n",
    "        # Save yearly file (just in case)\n",
    "        #filepath = Path(f'{pf}/data/input/allopt_{year}_extract.csv') # Set name \n",
    "        #data_1year.to_csv(filepath, index=False)\n",
    "\n",
    "        # Append to main data\n",
    "        df = pd.concat([df, data_1year])\n",
    "\n",
    "    # Close the connection\n",
    "    db.close()\n",
    "\n",
    "    # Save data file\n",
    "    filepath = Path(f'{pf}/data/input/rawdata_01jan1996to31dec2021_extract_{names[i]}.csv') # Set name \n",
    "    df.to_csv(filepath, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treasury rate data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloaded from FRED\n",
    "\n",
    "4 week (1 month) - https://fred.stlouisfed.org/series/DTB4WK\n",
    "\n",
    "3 months - https://fred.stlouisfed.org/series/DTB3\n",
    "\n",
    "Before 2001-07-31: uses 3mo\n",
    "\n",
    "On and after 2001-07-31: uses 1mo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_trate_1mo3mo = pd.read_excel(f'{pf}/data/input/trate_1mo3mo.xlsx', index_col=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S&P 500 dividend yield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data\n",
    "df_snp_dy =  pd.read_excel(f'{pf}/data/input/div_shiller.xls')  # from # http://www.econ.yale.edu/~shiller/data.htm   \n",
    "\n",
    "# resample with forward fill to put set the value of the month for each day \n",
    "df_snp_dy['date'] = pd.to_datetime(df_snp_dy.date, format='%d/%m/%Y')\n",
    "df_snp_dy = df_snp_dy.set_index('date').resample('D').ffill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## S&P 500 daily price data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Specify start and end date\n",
    "date_start = '1995-01-01'\n",
    "date_end = '2022-01-01'\n",
    "\n",
    "# Get data with YFinance\n",
    "df = yf.download('^GSPC', start=date_start, end=date_end)\n",
    "\n",
    "# Rename columns \n",
    "df['snp'] = df['Adj Close']\n",
    "df.index.names = ['date']\n",
    "\n",
    "# Calculate 21 day moving average\n",
    "df['snp21ma'] = df['snp'].rolling(21).mean()\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df = df.drop(columns=['Open','High', 'Low', 'Close', 'Volume', 'Adj Close'])\n",
    "\n",
    "# Save data file\n",
    "filepath = Path(f'{pf}/data/input/SP500rawdata_1995-01-01to2021-12-31_extract.csv') # Set name \n",
    "df.to_csv(filepath, index=True)\n",
    "\n",
    "df_snp = df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MORE DATA: Indices and stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = ['SPX','NDX','DJX', 'RUT', 'TSLA', 'AMZN', 'GOOGL']\n",
    "tickers = ['^GSPC','^NDX', '^DJI', '^RUT', 'TSLA', 'AMZN', 'GOOGL']\n",
    "\n",
    "# Specify start and end date for yfinance\n",
    "date_start = '1996-01-01'\n",
    "date_end = '2022-01-01'\n",
    "\n",
    "for i, (name, ticker) in enumerate(zip(names, tickers)):\n",
    "    print(name, ticker)\n",
    "    \n",
    "    # Get yfinance data\n",
    "\n",
    "    # Get data with YFinance\n",
    "    df = yf.download(ticker, start=date_start, end=date_end)\n",
    "\n",
    "    # Rename columns \n",
    "    df['snp'] = df['Adj Close']\n",
    "    df.index.names = ['date']\n",
    "    \n",
    "\n",
    "    # Calculate 21 day moving average\n",
    "    df['snp21ma'] = df['snp'].rolling(21).mean()\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    df = df.drop(columns=['Open','High', 'Low', 'Close', 'Volume', 'Adj Close'])\n",
    "\n",
    "    # Save data file\n",
    "    filepath = Path(f'{pf}/data/input/stockprices_{name}.csv') # Set name \n",
    "    df.to_csv(filepath, index=True)\n",
    "\n",
    "    # To use in formatting\n",
    "    df_snp = df\n",
    "    df_snp = df_snp.reset_index()\n",
    "    \n",
    "    ### Option data\n",
    "    \n",
    "    # Read downloaded data\n",
    "    df = pd.read_csv (f'{pf}/data/input/rawdata_01jan1996to31dec2021_extract_{name}.csv') # Data\n",
    "    \n",
    "    ### Format date columns\n",
    "    df['date'] =  pd.to_datetime(df['date'], format='%Y-%m-%d')\n",
    "    df['exdate'] =  pd.to_datetime(df['exdate'], format='%Y-%m-%d')\n",
    "    df_snp['date'] =  pd.to_datetime(df_snp['date'], format='%Y-%m-%d')\n",
    "\n",
    "    ### Merge with snp and treasury data\n",
    "    df = pd.merge(left=df, right=df_snp, left_on=\"date\", right_on=\"date\", how=\"left\")\n",
    "    df = pd.merge(left=df, right=df_snp_dy, left_on=\"date\", right_on=\"date\", how=\"left\")\n",
    "    df = pd.merge(left=df, right=df_trate_1mo3mo, left_on=\"date\", right_on=\"date\", how=\"left\")\n",
    "\n",
    "\n",
    "    ### Calculated variables\n",
    "    # Calculate tau (days to expiration)\n",
    "    df['tau']=df['exdate']-df['date'] # shows in X days\n",
    "    df['tau'] = (df['tau'] / np.timedelta64(1,'D')).astype(int) # as int\n",
    "\n",
    "    # Calculate price (avg of bid/ask)\n",
    "    df['price'] = (df['best_bid']+df['best_offer'])/2\n",
    "\n",
    "    # Calculate strike (divide by 1000)\n",
    "    df['strike'] = df['strike_price']/1000\n",
    "    \n",
    "    if name == 'DJX':\n",
    "        df['strike']=df['strike_price']/10\n",
    "\n",
    "    if name == 'AMZN': # Adjust for stock split\n",
    "        df['snp']=df['snp']*20\n",
    "\n",
    "    if name == 'GOOGL': # Adjust for stock split\n",
    "        df['snp']=df['snp']*20\n",
    "\n",
    "    if name == 'TSLA': # Adjust for stock split\n",
    "        df['snp']=df['snp']*3\n",
    "        \n",
    "    # Moving average dividend yield\n",
    "    df['dy_ma'] = df['dividend'] / df['snp21ma']\n",
    "\n",
    "    # Moneyness variable\n",
    "    df['money'] = np.log(df['strike']*np.exp(-(df['tr']-df['dy_ma'])*df['tau']/252)/df['snp'])\n",
    "\n",
    "    ### Filters\n",
    "    # Filter volume > 0\n",
    "    df = df[df.volume > 0]\n",
    "\n",
    "    # Filter bid/ask > 0.05\n",
    "    df = df[(df['best_bid'] >=0.05) | (df['best_offer'] >=0.05)]\n",
    "    df['spread']=df['best_offer']-df['best_bid']\n",
    "    df = df[df.spread>=0]\n",
    "\n",
    "    # Filter 8 < tau < 365\n",
    "    #df = df[df.tau >= 9]\n",
    "    df = df[df.tau <= 365]\n",
    "\n",
    "    # Filter impl. vol > 0\n",
    "    df = df.dropna(subset=['impl_volatility'])\n",
    "    df = df[df.impl_volatility > 0]\n",
    "\n",
    "    # Filter for non standard expiry dates \n",
    "    df['exdate_day'] = df.exdate.dt.day\n",
    "    df['exdate_weekday'] = df.exdate.dt.dayofweek\n",
    "    df = df[df['exdate_day'] < 27]\n",
    "    df = df[df['exdate_weekday'] >= 4]\n",
    "\n",
    "    # Filter for maturity groups\n",
    "    df['tau_years'] = df['tau']/365 # tau in years\n",
    "    #df['maturity_group'] = np.where(df['tau_years']<0.25, \"low\", np.where(df['tau_years']>0.5, \"high\", \"med\")) # group tau_years by, 0-0.25 = low, 0.25-0.50 = med, tau_years > 0.5 = high\n",
    "    df['maturity_group'] = np.where(df['tau_years']<8/365, \"vlow\",  np.where((df['tau_years']<0.25)&(df['tau_years']<0.25), \"low\", np.where(df['tau_years']>0.5, \"high\", \"med\"))) #\n",
    "\n",
    "    # create new table: count n by date, cp_flag, tau\n",
    "    df_tcount = df.groupby(['date', 'cp_flag', 'tau']).size() #.groupby(level=1).max()\n",
    "    df_tcount = df_tcount.to_frame(name='n')\n",
    "    df_tcount = df_tcount.reset_index()\n",
    "\n",
    "    # merge count to main table\n",
    "    df = pd.merge(left=df, right=df_tcount, left_on=['date','cp_flag', 'tau'], right_on=['date','cp_flag', 'tau'], how='left')\n",
    "\n",
    "    # keep only groups with n > 3\n",
    "    df = df[df['n'] > 3]\n",
    "\n",
    "    # Sort data\n",
    "    df = df.sort_values(by=['date','cp_flag','exdate', 'tau', 'strike'])\n",
    "\n",
    "    # Set index to date\n",
    "    df = df.set_index('date')\n",
    "    \n",
    "    # Save processed data file\n",
    "    filepath = Path(f'{pf}/data/output/allopt_01jan1996to31dec2021_extract_{name}.csv') # Set name \n",
    "    df.to_csv(filepath, index=True)\n",
    "    \n",
    "    print(name, ' - done', len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in names:\n",
    "    # Save processed data file by year\n",
    "    # full period\n",
    "    start_dates = ['1996-01-01','1997-01-01','1998-01-01','1999-01-01','2000-01-01','2001-01-01','2002-01-01','2003-01-01','2004-01-01','2005-01-01','2006-01-01','2007-01-01','2008-01-01','2009-01-01','2010-01-01','2011-01-01','2012-01-01', '2013-01-01', '2014-01-01', '2015-01-01', '2016-01-01', '2017-01-01',\n",
    "              '2018-01-01', '2019-01-01', '2020-01-01', '2021-01-01']\n",
    "    end_dates = ['1996-12-31','1997-12-31','1998-12-31','1999-12-31','2000-12-31','2001-12-31','2002-12-31','2003-12-31','2004-12-31','2005-12-31','2006-12-31','2007-12-31','2008-12-31','2009-12-31','2010-12-31','2011-12-31','2012-12-31', '2013-12-31', '2014-12-31', '2015-12-31', '2016-12-31', '2017-12-31',\n",
    "              '2018-12-31', '2019-12-31', '2020-12-31', '2021-12-31']\n",
    "\n",
    "    df_main = pd.read_csv (f'{pf}/data/output/allopt_01jan1996to31dec2021_extract_{name}.csv')  # full data set\n",
    "\n",
    "    #Yearly datasets\n",
    "    for start, end in zip(start_dates, end_dates):\n",
    "        ## Filter on date\n",
    "        df = df_main[df_main['date'].between(start, end)]\n",
    "\n",
    "        filepath = Path(f'{pf}/data/output/allopt_01Jan{start[0:4]}to31Dec{end[0:4]}_extract_{name}.csv') # Set name \n",
    "        df.to_csv(filepath, index=True)\n",
    "        print(name, len(df))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
