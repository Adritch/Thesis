{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30ed6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import scipy\n",
    "from scipy import optimize\n",
    "from IPython.display import display, Math, Latex\n",
    "from scipy import stats\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Parent folder\n",
    "import pathlib\n",
    "from pathlib import Path  \n",
    "pf = pathlib.Path().resolve() # Points to parent folder containing notebook and data, eg: 'C:/Users/Carla/Dropbox/Uni/10. Semester/Dynamic Programming/Term paper'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462f781d",
   "metadata": {},
   "source": [
    "# Step 1: Constrained Least Squares\n",
    "\n",
    "$$\n",
    "\\min _{\\left\\{m_{i}\\right\\}_{i=1}^{n} \\in \\mathbb{R}^{n}} \\sum_{i=1}^{n}\\left(m_{i}-y_{i}\\right)^{2}\n",
    "$$\n",
    "\n",
    "\\begin{equation}\n",
    "0 \\leq \\frac{m_{i+1}-m_{i}}{k_{i+1}-k_{i}} \\leq e^{-r \\tau} \\text { for } i=1, \\ldots, n-1 \\text { (monotonicity) }\n",
    "\\end{equation}\n",
    "\n",
    "(monotonicity constraint for puts shown, inequality reversed for calls)\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{m_{i+1}-m_{i}}{k_{i+1}-k_{i}} \\leq \\frac{m_{i+2}-m_{i+1}}{k_{i+2}-k_{i+1}} \\text { for } i=1, \\ldots, n-2 \\text { (convexity) }\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "Conducting a constrained least squares optimization on each cross section of option prices (over strike prices $k$, for fixed time $t$ and maturity $\\tau$)\n",
    "\n",
    "The problem of constrained least squares regression consists in finding the closest values $m_{i}$, in the sense of least squares, to a set of $n$ observations $y_{1},y_{2},...,y_{n}$ \n",
    "satisfying a set of constraints.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a26bb18",
   "metadata": {},
   "source": [
    "## Define objective and constraints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2770073b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(x):\n",
    "    \"\"\" Returns the squared difference between a value vector x and actual prices y, note that prices must be a global defined variable y \"\"\"\n",
    "    return sum((x-y)**2)\n",
    "\n",
    "def monotonicity_constraint(x):\n",
    "    \"\"\" Checks for monotonicity in prices, note that strikes k must be a global defined variable, and it also uses 'cp_flag' to tell if put or call! \"\"\"\n",
    "    dx = x[1:] - x[:len(x)-1]\n",
    "    dk = k[1:] - k[:len(k)-1]\n",
    "    \n",
    "    if cp_flag == 'P':\n",
    "        return disc[:len(disc)-1] - (dx/dk)  \n",
    "    if cp_flag == 'C':\n",
    "        return  disc[:len(disc)-1] + (dx/dk)  \n",
    "    \n",
    "def convexity_constraint(x):\n",
    "    \"\"\" Checks for convexity in prices \"\"\"\n",
    "    dx = x[1:] - x[:len(x)-1]\n",
    "    dk = k[1:] - k[:len(k)-1]\n",
    "    ddx = dx[1:]\n",
    "    ddk = dk[1:]\n",
    "    return ((ddx / ddk) - (dx[:len(x)-2] / dk[:len(k)-2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1b5427",
   "metadata": {},
   "source": [
    "## Run the regression for calls and puts, for each date and tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0bebe8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# full period\n",
    "start_dates = ['1996-01-01','1997-01-01','1998-01-01','1999-01-01','2000-01-01','2001-01-01','2002-01-01','2003-01-01','2004-01-01','2005-01-01','2006-01-01','2007-01-01','2008-01-01','2009-01-01','2010-01-01','2011-01-01','2012-01-01', '2013-01-01', '2014-01-01', '2015-01-01', '2016-01-01', '2017-01-01',\n",
    "          '2018-01-01', '2019-01-01', '2020-01-01', '2021-01-01']\n",
    "end_dates = ['1996-12-31','1997-12-31','1998-12-31','1999-12-31','2000-12-31','2001-12-31','2002-12-31','2003-12-31','2004-12-31','2005-12-31','2006-12-31','2007-12-31','2008-12-31','2009-12-31','2010-12-31','2011-12-31','2012-12-31', '2013-12-31', '2014-12-31', '2015-12-31', '2016-12-31', '2017-12-31',\n",
    "          '2018-12-31', '2019-12-31', '2020-12-31', '2021-12-31']\n",
    "\n",
    "names = ['SPX','NDX','DJX', 'RUT', 'TSLA', 'AMZN', 'GOOGL']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471b2dd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for name in names:\n",
    "    for start, end in zip(start_dates, end_dates):\n",
    "\n",
    "        # Read processed data file from optiondataprep (by year)\n",
    "        filepath = Path(f'{pf}/data/output/allopt_01Jan{start[0:4]}to31Dec{end[0:4]}_extract_{name}.csv') # Set name \n",
    "        df_main = pd.read_csv (filepath)  \n",
    "\n",
    "        df_main = df_main.sort_values(by=['date','exdate', 'tau', 'strike'])\n",
    "\n",
    "        #Ensure date is filtered correctly\n",
    "        df_main = df_main[df_main['date'].between(start, end)]\n",
    "\n",
    "        df_main['p_adj'] = np.nan # Create nan column for p_adj to fill with regression predictions\n",
    "        nrows = len(df_main)\n",
    "\n",
    "        if nrows > 1: # Continue with estimation\n",
    "\n",
    "            # All combinations of flag, date, tau that we need to calculate the regression for\n",
    "            loop_vars = df_main.groupby(['cp_flag','date','tau']).size().reset_index()\n",
    "            length = len(loop_vars)\n",
    "\n",
    "            # Estimation loop\n",
    "            for i, (cp_flag, date, tau) in enumerate(tqdm(zip(loop_vars['cp_flag'], loop_vars['date'], loop_vars['tau']), total=length, position=0, leave=True, desc=start[0:4])): # For loop with progress bar\n",
    "\n",
    "                ### Setup data and variables for regression\n",
    "\n",
    "                # Select fixed date (date)\n",
    "                df = df_main[df_main.date == date]\n",
    "\n",
    "                # Select put or call only (cp_flag )\n",
    "                df = df[df['cp_flag'] == cp_flag]\n",
    "\n",
    "                # Select fixed maturity (tau)\n",
    "                df = df[df['tau'] == int(tau)]\n",
    "\n",
    "                if len(df) < 3: # Should not be neccessary if passing full data set as this filter is already applied earlier, but is a must for testing otherwise regression will fail\n",
    "                    continue\n",
    "\n",
    "                else:\n",
    "\n",
    "                    ### Regression\n",
    "\n",
    "                    # Setup regression variables\n",
    "                    y = df['price'].values # Actual price\n",
    "                    k = df['strike'].values # Strikes\n",
    "                    r = df['dy_ma'].values[0] # Risk free rate\n",
    "                    tau = df['tau'].values[0] # Tau\n",
    "                    tau_years = df['tau_years'].values[0] # Tau\n",
    "                    #disc = np.exp(-df['dy_ma'].values[0]*df[\"tau_years\"].values)\n",
    "                    disc = np.exp(-df['tr'].values[0]*df[\"tau_years\"].values)\n",
    "\n",
    "                    # Setup bounds and constraints\n",
    "                    bounds = scipy.optimize.Bounds(lb= 0, ub=np.inf, keep_feasible=False) # lower bound = 0, upper bound = max price * 10\n",
    "                    c1 = {'type': 'ineq', 'fun': monotonicity_constraint} \n",
    "                    c2 = {'type': 'ineq', 'fun': convexity_constraint}\n",
    "\n",
    "                    # Initial values:\n",
    "                    x0 = y+0.001\n",
    "\n",
    "                    # Call optimizer\n",
    "                    result = optimize.minimize(objective,x0, method='SLSQP', bounds=bounds, constraints=[c1, c2], options={'disp':False, 'ftol':1e-7, 'maxiter':1000})\n",
    "\n",
    "                    # Collect results along with date, flag, tau and strike in one data frame\n",
    "                    frames = [df['date'], df['cp_flag'], df['tau'], df['strike'], pd.DataFrame(result.x, columns = ['p_adj'], index=df.index)] \n",
    "                    df_results = pd.concat(frames, axis=1, ignore_index=True)\n",
    "                    df_results.columns = ['date', 'cp_flag', 'tau', 'strike', 'p_adj']\n",
    "\n",
    "                    # Update main data frame with results (p_adj)\n",
    "                    df_main.update(df_results)\n",
    "\n",
    "            # Add column with predicted price vs actual price\n",
    "            df_main['p_adj_error_pct']= (df_main['p_adj']-df_main['price'])/df_main['price']*100\n",
    "\n",
    "            # Save the results\n",
    "            filepath = Path(f'{pf}/data/output/cls_regression_nrows_{start[0:4]}-{end[0:4]}_{name}.csv') # Set name \n",
    "            df_main.to_csv(filepath, index=False)\n",
    "        else:\n",
    "            print(f'No data for {name}, {start[0:4]}')\n",
    "            continue\n",
    "    print(name, ' - done', len(df))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d99ff7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for name in names:\n",
    "    df_all = pd.DataFrame()\n",
    "    # Merge data from each year to one file\n",
    "    for start, end in zip(start_dates, end_dates):\n",
    "        filepath = Path(f'{pf}/data/output/cls_regression_nrows_{start[0:4]}-{end[0:4]}_{name}.csv') # Set name \n",
    "        \n",
    "        try:\n",
    "            df_results = pd.read_csv(filepath)\n",
    "        except FileNotFoundError:\n",
    "            print(f'No data for {name}, {start_dates[0:4]}, {end_dates[0:4]}')\n",
    "            continue\n",
    "        \n",
    "       #df_results = pd.read_csv(filepath)\n",
    "        df_all = pd.concat((df_all, df_results))\n",
    "\n",
    "    # Save the results\n",
    "    filepath = Path(f'{pf}/data/output/cls_regression_nrows_{start_dates[0][0:4]}-{end_dates[-1][0:4]}_{name}.csv') # Set name \n",
    "    df_all.to_csv(filepath, index=False)\n",
    "    \n",
    "    # Checking for error in actual price vs predicted price\n",
    "    print(name, 'error pct>50: ',len(df_all[df_all.p_adj_error_pct>50]), 'total: ',len(df_all))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f34c128",
   "metadata": {},
   "source": [
    "# Step 2: Local Polynomial Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe24779c",
   "metadata": {},
   "source": [
    "We want to obtain the CDF of the state-price distribution $Q^*(s)$\n",
    "\n",
    "$Q^*(s)$ is connected to the first order derivative of the pricing functions (for puts and calls respectively)\n",
    "\n",
    "\\begin{aligned}\n",
    "&\\hat{Q}_p^*(s):=e^{r \\tau} \\hat{m}_p^{(1)}(s) \\\\\n",
    "&\\hat{Q}_c^*(s):=e^{r \\tau} \\hat{m}_c^{(1)}(s)+1\n",
    "\\end{aligned}\n",
    "\n",
    "To obtain first order derivatives (dropping subscript here as its the same) ${m}^{(1)}(s)$ we need to fit a polynomial on the strike prices and the adjusted option prices (p_adj)\n",
    "\n",
    "nonparametric local polynomial fit (Fan and Yao 2005)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "**Process :**\n",
    "1. Local nonparametric polynomial fit of order 2  - Estimating the beta matrix $\\begin{equation}\n",
    "\\hat{\\beta}:=\\left(X^{\\prime} W X\\right)^{-1} X^{\\prime} W y\n",
    "\\end{equation}$, etc\n",
    "2. Return the first derivative $\\hat{m}_o^{(1)}(s)$, standard error, and optimal bandwitdh etc\n",
    "3. Estimate the CDF state-price distribution $\\hat{Q}_o^*(s):=e^{r \\tau} \\hat{m}_o^{(1)}(s)+1\\{o=c\\} \\quad \\text { for } o=c, p$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b60520",
   "metadata": {},
   "source": [
    "## Local nonparametric polyfit function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7ce0d77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Utility function for newey-west covariance\n",
    "def phi(x):\n",
    "    return np.exp((-x**2)/2) / np.sqrt(2*np.pi)\n",
    "\n",
    "# Utility function for newey-west covariance\n",
    "def nw_cov(q, m):\n",
    "    mu = np.mean(q)\n",
    "    n = len(q)\n",
    "    gam = np.zeros(int(m))\n",
    "    cov = 0\n",
    "\n",
    "    for j in range(1, int(m)+1):\n",
    "        A = q[0:n-j]-mu\n",
    "        B = q[0+j:n]-mu\n",
    "        gam[j-1] = sum(A * B) /n\n",
    "        cov += (1-j/(m+1))*gam[j-1]\n",
    "    return(cov)\n",
    "\n",
    "# Utility function to make cdf and se valid\n",
    "def make_valid(qcdf_out, qcdf_se_out):\n",
    "    # Empty arrays\n",
    "    qcdf = np.zeros(len(qcdf_out))\n",
    "    qcdf_se_pt = np.zeros(len(qcdf_out))\n",
    "    \n",
    "    # Make Q and SE valid\n",
    "    for i in range(len(qcdf)):\n",
    "        qcdf[i] =  min(1, max(0, qcdf_out[i]))\n",
    "        if i > 1 and qcdf[i-1] == 1:\n",
    "            qcdf[i] = 1\n",
    "        if qcdf[i] == 0 or qcdf[i] == 1:\n",
    "            qcdf_se_pt[i] = 0\n",
    "        else:\n",
    "            qcdf_se_pt[i] = qcdf_se_out[i]\n",
    "            \n",
    "    return(qcdf, qcdf_se_pt)\n",
    "    \n",
    "def lpoly(kin, vk, vc, p, opth, h0):\n",
    "    \"\"\" \n",
    "    input variables: \n",
    "    # kin   current strike price (scalar)\n",
    "    # vk    vector of strikes (column vector, nk x 1)\n",
    "    # vc    vector of option prices (column vector, nk x 1)\n",
    "    # p     power p \n",
    "    # opth  switch for bandwidth \n",
    "    #           0 = use h0\n",
    "    #           1 = optimal local bandwidth\n",
    "    #           2 = optimal global bandwidth\n",
    "    # h0    user-specified bandwidth\n",
    "\n",
    "    # output variables:\n",
    "    # beta    local polynomial estimate and first derivative (p+1)x1\n",
    "    # beta_se standard error\n",
    "    # h     optimal bandwidth\n",
    "    # sign  singular matrix \"\"\"\n",
    "    \n",
    "    \n",
    "    # Setting initital hcon\n",
    "    if p == 1:\n",
    "        hcon = 0.776\n",
    "    elif p == 2:\n",
    "        hcon = 0.884\n",
    "    elif p == 3:\n",
    "        hcon = 1.006\n",
    "\n",
    "    # Number of strikes\n",
    "    nk=len(vk)\n",
    "    \n",
    "    # Setting\n",
    "    hnumsd = 3\n",
    "    sign = 0\n",
    "\n",
    "    ### Optimal bandwidth\n",
    "    if opth == 1 or opth == 2:\n",
    "        x = np.ones((nk,1))\n",
    "        d = 0 \n",
    "\n",
    "        while d<p+3:\n",
    "            d = d + 1\n",
    "            x = np.concatenate((x, vk[:, None]**d), axis=1)\n",
    "            \n",
    "        A = (np.transpose(x) @ x)\n",
    "        B =  np.transpose(x) @ vc\n",
    "        alpha = np.linalg.solve(A, B)\n",
    "\n",
    "        res = vc - (x@alpha)\n",
    "        ssr = sum(res**2)\n",
    "\n",
    "        w0 = lambda v=None: (v > np.mean(vk) - 1.5 *  np.std(vk)) * (v < np.mean(vk) + 1.5*np.std(vk))\n",
    "        mp1 = lambda v=None: math.factorial(p+1)*alpha[p+1] + 1/2*math.factorial(p+2)*alpha[p+2]*v +1/6*math.factorial(p+3)*alpha[p+3]*v**2 # Evaluate 3rd derivative\n",
    "        wmp1= sum(mp1(vk)**2 * w0(vk))\n",
    "\n",
    "        h = h0 \n",
    "\n",
    "    ### Compute local polynomial estimate\n",
    "    XX=np.ones((nk,1))\n",
    "    d=0\n",
    "    nu=1\n",
    "\n",
    "    while d < p:\n",
    "        d=d + 1\n",
    "        d_fact = math.factorial(d)\n",
    "\n",
    "        # Create matrices\n",
    "        XX=np.column_stack((XX, (vk - kin) ** d)) # Matrix: 1, (X-x0)^1, (X-x0)^2, ... , (X-x0)^p  \n",
    "        nu=np.vstack((nu, d_fact))\n",
    "\n",
    "    # Calculate beta as the solution to the linear matrix equation # X^T W X * beta = X^T W y.\n",
    "    W=np.diag(phi((vk - kin) / h) / h) # W is a (p+ 1)x(p+ 1) diagonal matrix whose i'th element is Kh(ki-s) (Kh(x) =h^-1*K(x/h), where K is the kernel and h is the bandwidth\n",
    "    A = (np.transpose(XX) @ W) @ XX\n",
    "    B = (np.transpose(XX) @ W) @ vc\n",
    "    beta = np.linalg.solve(A , B) # the derivative m′(x) is estimated by the local slope ratherthan the derivative of the estimated regression function - b[1] is the derivative\n",
    "\n",
    "\n",
    "    ### Compute standard error of local polynomial estimate\n",
    "    ps = 0 # pow for smoothing s2 hat\n",
    "    #hs = h # h for smoothing s2 hat \n",
    "    hs = h0 # h for smoothing s2 hat \n",
    "\n",
    "    # Computing Sigma (S)\n",
    "    A = np.linalg.solve(nu**2*(np.transpose(XX) @ W) @ XX, ((np.transpose(XX) @ W) @ W) @ XX)\n",
    "    B = (np.transpose(XX) @ W) @ XX\n",
    "    S = np.linalg.solve(B.conj().T, A.conj().T).conj().T # Answer 2 -  https://stackoverflow.com/questions/1007442/mrdivide-function-in-matlab-what-is-it-doing-and-how-can-i-do-it-in-python\n",
    "    \n",
    "    # Squared residuals\n",
    "    resid2 = (vc - beta[0])**2  \n",
    "    resid2[np.isnan(resid2)]=0\n",
    "    resid2[np.isinf(resid2)]=0\n",
    "\n",
    "    XX=np.ones((nk,1))\n",
    "    d=0\n",
    "    nu=1\n",
    "    while d < ps:\n",
    "        d=d + 1\n",
    "        kk = (vk - kin)**d    \n",
    "        XX = np.concatenate((XX, kk[:, None]), axis=1)\n",
    "        d_fac = scipy.special.factorial(d)\n",
    "        nu = np.vstack((nu, d_fac))\n",
    "    \n",
    "    W = np.diag(phi((vk-kin)/hs)/hs)  \n",
    "    A = ((np.transpose(XX) @ W) @ XX)\n",
    "    B = (np.transpose(XX) @ W) @ resid2\n",
    "    s2hat = np.transpose(nu) * np.linalg.solve(A, B) # (ps+1) x 1\n",
    "    beta_var = s2hat*S\n",
    "    beta_se = np.real(np.sqrt(np.real(np.diag(beta_var))))\n",
    "\n",
    "    # Optimal local bandwidth\n",
    "    if opth == 1:        \n",
    "        fhat = np.mean(phi((vk-kin)/hs)/hs)\n",
    "        h = hcon*(np.mean(resid2)/mp1(kin)**2/fhat/nk)**(1/(2*p+3))        \n",
    "\n",
    "    return(beta, beta_se, h, sign)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c783ccfa",
   "metadata": {},
   "source": [
    "# Step 3: Point and Interval Estimation of Bubbles\n",
    "\n",
    "We define a function that cmputes cdf and se based on local polynomial estimation, the function will evaluate over a grid of strikes from the lowest\n",
    "     strike to the highest strike on the selected date, where ds (delta S) is the step size in the grid\n",
    "   \n",
    "For each date, tau for puts and calls respectively we compute the cdf estimates and asset value:\n",
    "1. Using Lpoly - estimate the CDF state-price distribution $\\hat{Q}_o^*(s):=e^{r \\tau} \\hat{m}_o^{(1)}(s)+1\\{o=c\\} \\quad \\text { for } o=c, p$\n",
    "2. Normalize CDF by equation (12): $Q^{\\dagger}(s):=\\frac{Q^*(s)-Q^*(\\ell)}{Q^*(u)-Q^*(\\ell)} \\quad \\text { for } s \\in[\\ell, u]$\n",
    "3. Estimate the fundamental asset value by (17): $\\hat{\\mu}^{\\dagger}:=e^{-r \\tau} \\ell+e^{-r \\tau} \\sum_{i=1}^{n_s}\\left[1-\\hat{Q}^{\\dagger}\\left(s_i\\right)\\right] \\Delta s_i$ \n",
    "...\n",
    "4. Compute the standard error of estimates\n",
    "\n",
    "We then combine the estimates and compute bubbles:\n",
    "1. Compute the weighted estimate of the fundamental asset value $\\hat{\\mu}^{\\dagger}=\\omega \\hat{\\mu}_c^{\\dagger}+(1-\\omega) \\hat{\\mu}_p^{\\dagger}(\\omega=[0,1])$ where the **weights are the proportion of put/call contracts** , and the standard error of the estimate\n",
    "2. Compute the bias $\\hat{B}$\n",
    "3. Compute the bubble (uncorrected) by $\\hat{\\Pi}^S=S+\\hat{B}-\\hat{\\mu}^{\\dagger}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743b4792",
   "metadata": {},
   "source": [
    "## Define function for computing cdf estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9d41541",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_cdf_estimates(df_main, cp_flag, date, tau, ds):\n",
    "    \"\"\" Computes cdf and se based on local polynomial estimation, the function will evaluate over a grid of strikes from the lowest\n",
    "     strike to the highest strike on the selected date, where ds (delta S) is the step size in the grid\n",
    "     \n",
    "    input variables: \n",
    "    df        df containing optiondata with CLS adjusted prices\n",
    "    cp_flag   switch for put/call\n",
    "    date      fixed date for run estimation\n",
    "    tau       fixed tau for estimation\n",
    "    \n",
    "    output variables:\n",
    "    qcdf          cumulative distribution function\n",
    "    qcdf_se_out   standard errror of cdf (before correction) \"\"\"\n",
    "    \n",
    "    \n",
    "     # Select fixed date (date)\n",
    "    df = df_main[df_main['date'] == date]\n",
    "    \n",
    "    # Drop observations where we dont have estimate for p_adj\n",
    "    df = df[df['p_adj'].notna()]\n",
    "\n",
    "    # Select fixed maturity (tau)\n",
    "    df = df[df['tau'] == int(tau)]\n",
    "    \n",
    "    # Select put or call only (cp_flag )\n",
    "    df = df[df['cp_flag'] == cp_flag]\n",
    "        \n",
    "    # Evaluate over range of strikes\n",
    "    ds = ds # step size - delta s\n",
    "    lb = min(df['strike']) # lower bound of strike range\n",
    "    ub = max(df['strike']) # Upper bound of strike range\n",
    "\n",
    "    k_range = np.arange(lb, ub+ds, ds)\n",
    "    \n",
    "    #print(f'lb{lb}, ub{ub}, k_range {len(k_range)}')\n",
    "\n",
    "    # Discount factor\n",
    "    #dis = np.exp(-df['dy_ma'].values[0]*df[\"tau_years\"].values[0])\n",
    "    dis = np.exp(-df['tr'].values[0]*df[\"tau_years\"].values[0])\n",
    "    \n",
    "\n",
    "    # -- # \"Calculate qcdf_put by equation (14) \" # -- #\n",
    "    # Local polynomial fit -> Evaluate qcdf with strikes - (first derivative from lpoly) \n",
    "    vk = np.array(df['strike'])\n",
    "    vc = np.array(df['p_adj'])\n",
    "\n",
    "    # Local regression parameters\n",
    "    p = 2\n",
    "    hnumsd = 3 #number of mean dk set for h0 (initial bandwidth) \n",
    "    opth = 1\n",
    "\n",
    "    # Initial bandwitdh\n",
    "    dk = df['strike'].diff()\n",
    "    hx0 = np.mean(dk)*hnumsd\n",
    "\n",
    "    qcdf_out, qcdf_se_out = [], []\n",
    "    for i, kin in enumerate(k_range):\n",
    "        \n",
    "        # Run estimation\n",
    "        b, b_se, h, sign = lpoly(kin, vk, vc, p, opth, hx0) # to get h opt\n",
    "        b, b_se, h, sign = lpoly(kin, vk, vc, p, opth, h) # with updated h\n",
    "        \n",
    "\n",
    "        if cp_flag == 'P':\n",
    "            qcdf_out.append(b[1]*dis**(-1))\n",
    "            qcdf_se_out.append(b_se[1]*dis**(-1))\n",
    "\n",
    "        elif cp_flag == 'C':\n",
    "            qcdf_out.append(1+b[1]*dis**(-1))\n",
    "            qcdf_se_out.append(b_se[1]*dis**(-1))\n",
    "\n",
    "\n",
    "    # make Q and se valid\n",
    "    qcdf, qcdf_se_pt = make_valid(qcdf_out, qcdf_se_out)\n",
    "\n",
    "    # -- # \"Normalize cdf by equation (12)\" # -- #\n",
    "    qcdf_norm = (qcdf - min(qcdf))/(max(qcdf)-min(qcdf))\n",
    "\n",
    "    # -- # \"Calculate estimate of fundamental value by equation (17)\" # -- #\n",
    "    qcdf_mu = (lb + np.nansum((1-qcdf_norm)*ds))*dis\n",
    "    qcdf_mu = (np.nansum(1 - qcdf_norm) * ds + vk[0])*dis\n",
    "\n",
    "    # Compute standard error of estimate\n",
    "    mp = np.ceil(len(k_range)**0.25)\n",
    "    wp = ((np.ones(len(k_range)) * ds)* dis) / (max(qcdf)-min(qcdf))\n",
    "    \n",
    "    if cp_flag == 'P':\n",
    "        wp[0] = (dis*ds*sum(qcdf) - (df['price'].values[-1] - df['price'].values[0])) / (max(qcdf)-min(qcdf))**2 + dis*ds / (max(qcdf)-min(qcdf))\n",
    "        wp[-1] = -(dis*ds*sum(qcdf) - (df['price'].values[-1] - df['price'].values[0])) / (max(qcdf)-min(qcdf))**2 + dis*ds / (max(qcdf)-min(qcdf))\n",
    "    if cp_flag == 'C':\n",
    "        wp[0] = (dis*ds*sum(qcdf) - (df['price'].values[-1] - df['price'].values[0]+dis*(vk[-1]-vk[0]) )) / (max(qcdf)-min(qcdf))**2 + dis*ds / (max(qcdf)-min(qcdf))\n",
    "        wp[-1] = -(dis*ds*sum(qcdf) - (df['price'].values[-1] - df['price'].values[0]+dis*(vk[-1]-vk[0]))) / (max(qcdf)-min(qcdf))**2 + dis*ds / (max(qcdf)-min(qcdf))\n",
    "\n",
    "\n",
    "    qcdf_norm_se_pt = qcdf_se_pt * wp \n",
    "    qcdf_pt = qcdf * wp\n",
    "    qcdf_se=np.sqrt(np.nansum(qcdf_norm_se_pt ** 2) + 2*nw_cov(qcdf_pt, mp))\n",
    "\n",
    "    # Additional returns\n",
    "    n = len(df) # number put or calls in this subset of data\n",
    "    vol = sum(df['volume']) # volume\n",
    "\n",
    "    #print(f' The estimated fundamental value by {cp_flag, date, tau} is {qcdf_mu:.2f} with standard error {qcdf_se:.2f}')\n",
    "\n",
    "    return(qcdf, qcdf_mu, qcdf_se, n, dis, vol, df, sign) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903da944",
   "metadata": {},
   "source": [
    "## Run estimation for each date, tau (intersect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40e09e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weighting scheme\n",
    "weights = [False, True, 'Naive'] #Volume weighted = True / False, if false then jarrow weights\n",
    "volume_weighted = 'True' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5435ab1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for name in names:\n",
    "    for start, end in zip(start_dates, end_dates):\n",
    "        # Read data (from step 1 - CLS)\n",
    "        filepath = Path(f'{pf}/data/output/cls_regression_nrows_{start[0:4]}-{end[0:4]}_{name}.csv') # Set name \n",
    "        try:\n",
    "            df_main = pd.read_csv(filepath)\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f'No data for {name}, {start[0:4]}')\n",
    "            continue\n",
    "            \n",
    "\n",
    "        if len(df_main) > 1: # Continue with estimation\n",
    "\n",
    "            # All combinations of date, tau that we need to run the estimations for\n",
    "            loop_vars = df_main.groupby(['date','tau']).size().reset_index()\n",
    "            length = len(loop_vars)\n",
    "            variables = ['date', 'sout', 'tau', 'qcdf_mu', 'qcdfp_mu', 'qcdfc_mu','sbub_qcdf', 'sbub_qcdfp', 'sbub_qcdfc', 'otmc', 'call1', 'sbub_qcdfp_se', 'sbub_qcdfc_se', 'sbub_qcdf_se', 'qcdfp_bias',\n",
    "                   'qcdfc_bias', 'qcdf_bias', 'qcdf_A_lb', 'qcdf_A_ub', 'qcdf_Ap_lb', 'qcdf_Ap_ub', 'qcdf_Ac_lb', 'qcdf_Ac_ub' ,\n",
    "                   'qcdf_B1', 'qcdf_B21', 'qcdf_B22', 'qcdf_B23','qcdf_B3', 'Bcbub_lb', 'Bcbub_ub', 'scene', 'qcdfp_lb', 'qcdfp_ub',\n",
    "                   'qcdfc_lb', 'qcdfc_ub', 'lp', 'up', 'lc', 'uc', 'nkc', 'nkp', 'sumvolc', 'sumvolp', 'B1p', 'B1c', 'B2p', 'B2c', 'sign']\n",
    "\n",
    "            # For saving results\n",
    "            results = np.zeros((length, len(variables)-1)) #nrows, ncolums (number of variables)\n",
    "            index, dates = [], []\n",
    "\n",
    "            for i, (date, tau) in enumerate(tqdm(zip(loop_vars['date'], loop_vars['tau']), total=length, position=0, leave=True, desc=start[0:4])): # For loop with progress bar\n",
    "                # Set date and tau (loop)\n",
    "                date, tau = date, tau\n",
    "\n",
    "                ### Setup data and variables for estimation\n",
    "\n",
    "                # Select fixed date (date)\n",
    "                df = df_main[df_main['date'] == date]\n",
    "\n",
    "                # Select fixed maturity (tau)\n",
    "                df = df[df['tau'] == int(tau)]\n",
    "                \n",
    "                # For k-range - fineness of evaluation grid\n",
    "                ds = np.mean(np.diff(df['strike']))\n",
    "                \n",
    "                # Count number of observations for each cpflag, tau , if either is less than or equal to 3 then skip this date:\n",
    "                ptaucount = df[df.cp_flag == 'P'].shape[0]\n",
    "                ctaucount = df[df.cp_flag == 'C'].shape[0]\n",
    "                if ptaucount <= 3 or ctaucount <= 3:\n",
    "                    index.append(i), dates.append('not enough observations')\n",
    "                    continue\n",
    "\n",
    "                else:\n",
    "                    sign = 0\n",
    "\n",
    "                    # Compute individual estimates using calls and puts\n",
    "                    qcdfp, qcdfp_mu, qcdfp_se, nputs, dis, volp, df_p, sign = compute_cdf_estimates(df, 'P', date, tau, ds) # put estimates\n",
    "                    qcdfc, qcdfc_mu, qcdfc_se, ncalls, dis, volc, df_c, sign = compute_cdf_estimates(df, 'C', date, tau, ds) # call estimates\n",
    "\n",
    "                    # Weights used - currently number of puts/calls for give date, tau\n",
    "                    if volume_weighted == True:\n",
    "                        nputs = volp\n",
    "                        ncalls = volc\n",
    "                        n = nputs+ncalls\n",
    "                    if volume_weighted == 'Naive':\n",
    "                        nputs = 1\n",
    "                        ncalls = 1\n",
    "                        n = nputs+ncalls\n",
    "\n",
    "                    # Combined estimate\n",
    "                    n = nputs + ncalls # \n",
    "                    qcdf_mu = nputs/n*qcdfp_mu + ncalls/n*qcdfc_mu        \n",
    "                    qcdf_se = np.sqrt((nputs/n*qcdfp_se)**2 + (ncalls/n*qcdfc_se)**2)\n",
    "\n",
    "                    # Set up strike and price arrays for bias calculations\n",
    "                    if df_p.size == 0:\n",
    "                        pk = np.empty(0)\n",
    "                        put = np.empty(0)\n",
    "                    else:\n",
    "                        pk = df_p['strike'].values\n",
    "                        put = df_p['price'].values\n",
    "\n",
    "                    if df_c.size == 0:\n",
    "                        ck = np.empty(0)\n",
    "                        call = np.empty(0)\n",
    "                    else:\n",
    "                        ck = df_c['strike'].values\n",
    "                        call = df_c['price'].values\n",
    "\n",
    "                    # Find individual biases\n",
    "                    try:\n",
    "                        lc_p = np.argwhere(pk>=ck[0])[0]\n",
    "                    except IndexError:\n",
    "                        lc_p = np.argwhere(pk<=ck[0])[-1]\n",
    "                    try:\n",
    "                        up_c = np.argwhere(ck<=pk[-1])[-1]\n",
    "                    except IndexError:\n",
    "                        up_c = np.argwhere(ck>=pk[-1])[0]\n",
    "                    try:\n",
    "                        lp_c = np.argwhere(ck>=pk[0])[0]\n",
    "                    except IndexError:\n",
    "                        lp_c = np.argwhere(ck<=pk[0])[-1]\n",
    "                    try:\n",
    "                        uc_p = np.argwhere(pk<=ck[-1])[-1]\n",
    "                    except IndexError:\n",
    "                        uc_p = np.argwhere(pk>=ck[-1])[0]\n",
    "\n",
    "                    B1p = dis * min(qcdfp) * (pk[-1] - pk[0]) / (max(qcdfp)-min(qcdfp))\n",
    "                    B1c = dis * min(qcdfc) * (ck[-1] - ck[0]) / (max(qcdfc)-min(qcdfc))\n",
    "\n",
    "                    B2p = (1/(max(qcdfp)-min(qcdfp))-1)*(put[-1] - put[0])\n",
    "                    B2c = (1/(max(qcdfc)-min(qcdfc))-1)*(dis*(ck[-1]-ck[0]) + call[-1] - call[0])\n",
    "\n",
    "                    A_lb = -put[0]\n",
    "                    A_ub = call[-1]\n",
    "                    Ap_lb = -put[0]\n",
    "                    Ap_ub = call[up_c]\n",
    "                    Ac_lb = -put[lc_p]\n",
    "                    Ac_ub = call[-1]\n",
    "\n",
    "                    # Compute bias for combined put and call estimate\n",
    "                    B1 = (nputs*B1p + ncalls*B1c)/n\n",
    "                    B21 = (nputs*B2p + ncalls*B2c)/n\n",
    "                    B3 = nputs/n*dis*(ck[-1]-pk[-1])\n",
    "\n",
    "                    if pk[0] <= ck[0] and pk[-1] <= ck[-1] and ck[0] <= pk[-1]:\n",
    "                        B22 = nputs/n*(dis*(ck[-1]-pk[-1]) + call[up_c] - call[up_c])\n",
    "                        B23 = ncalls/n*(put[lc_p] - put[0])\n",
    "                        scene = 1\n",
    "\n",
    "                    elif ck[0] <= pk[0] and ck[-1] <= pk[-1] and pk[0] <= ck[-1]:\n",
    "                        B22 = -nputs/n*(put[-1] - put[uc_p])\n",
    "                        B23 = -ncalls/n*(dis*(pk[0]-ck[0]) + call[lp_c] - call[0])\n",
    "                        scene = 2\n",
    "\n",
    "                    elif pk[0] <= ck[0] and ck[-1] <= pk[-1]:\n",
    "                        B22 = ncalls/n*(put[lc_p] - put[0])\n",
    "                        B23 = -nputs/n*(put[-1] - put[uc_p])\n",
    "                        scene = 3\n",
    "\n",
    "                    elif ck[0] <= pk[0] and pk[-1] <= ck[-1]:    \n",
    "                        B22 = -ncalls/n*(dis*(pk[0]-ck[0]) + call[lp_c] - call[0])\n",
    "                        B23 = nputs/n*(dis*(ck[-1]-pk[-1]) + call[-1] - call[up_c])\n",
    "                        scene = 4\n",
    "\n",
    "                    elif pk[-1] < ck[0]:\n",
    "                        B22 = ncalls/n*(put[-1] - put[0])\n",
    "                        B23 = nputs/n*(dis*(ck[-1]-ck[0]) + call[-1] - call[0])\n",
    "                        A_ub = call[-1] + dis\n",
    "                        scene = 5\n",
    "                    else:\n",
    "                        scene = 6\n",
    "\n",
    "\n",
    "                    ### Set up output variables\n",
    "                    qcdfp_bias = B1p - B2p                \n",
    "                    qcdfc_bias = B1c - B2c\n",
    "                    qcdf_bias = B1 - B21 + B22 + B23 - B3\n",
    "                    qcdf_A_lb = A_lb\n",
    "                    qcdf_A_ub = A_ub\n",
    "                    qcdf_Ap_lb = Ap_lb\n",
    "                    qcdf_Ap_ub = Ap_ub\n",
    "                    qcdf_Ac_lb = Ac_lb\n",
    "                    qcdf_Ac_ub = Ac_ub\n",
    "                    qcdf_B1 = B1\n",
    "                    qcdf_B21 = B21\n",
    "                    qcdf_B22 = B22\n",
    "                    qcdf_B23 = B23\n",
    "                    qcdf_B3 = B3\n",
    "\n",
    "                    # Compute bubble\n",
    "                    sout = df['snp'].values[0] # Underlying price\n",
    "                    sbub_qcdfp = sout - qcdfp_mu  # 100% put information\n",
    "                    sbub_qcdfc = sout - qcdfc_mu # 100% call information\n",
    "                    sbub_qcdf = sout - qcdf_mu  # weighted bubble estimate\n",
    "                    \n",
    "                    # Standard errors\n",
    "                    sbub_qcdfp_se = qcdfp_se \n",
    "                    sbub_qcdfc_se = qcdfc_se \n",
    "                    sbub_qcdf_se = qcdf_se\n",
    "                    #\n",
    "                    qcdfp_lb = min(qcdfp)\n",
    "                    qcdfp_ub = max(qcdfp)\n",
    "                    qcdfc_lb = min(qcdfc)\n",
    "                    qcdfc_ub = max(qcdfc)\n",
    "\n",
    "                    lp = pk[0]\n",
    "                    up = pk[-1]\n",
    "                    lc = ck[0]\n",
    "                    uc = ck[-1]\n",
    "                    nkp = nputs\n",
    "                    nkc = ncalls\n",
    "                    sumvolc = volc\n",
    "                    sumvolp = volp\n",
    "                    otmc = call[-1]\n",
    "                    call1 = call[0]\n",
    "                    Bcbub_lb = -(1/(max(qcdfc)-min(qcdfc))-1)*call[0]\n",
    "                    Bcbub_ub = -(1/(max(qcdfc)-min(qcdfc))-1)*call[-1]\n",
    "\n",
    "                     # save results\n",
    "                    index.append(i), dates.append(date)\n",
    "                    for j, var in enumerate(variables[1:]):\n",
    "                        results[i, j] = globals()[var]\n",
    "                        \n",
    "                    #print(f'df_main {start[0:4]}, {lp}, {up}')\n",
    "\n",
    "            # Format results\n",
    "            df_dates = pd.DataFrame(dates)\n",
    "            df_results = pd.DataFrame(results)\n",
    "            df_results = pd.concat([df_dates, df_results], axis=1)\n",
    "            df_results.columns = variables\n",
    "            df_results=df_results[df_results.date != 'not enough observations']\n",
    "            df_results['date'] =  pd.to_datetime(df_results['date'], format='%Y-%m-%d')\n",
    "\n",
    "            # Save estimation results\n",
    "            filepath = Path(f'{pf}/data/output/bub_interval_estimates_nrows_{start[0:4]}-{end[0:4]}_VW{volume_weighted}_{name}.csv') # Set name \n",
    "            df_results.to_csv(filepath, index=True)\n",
    "        else:\n",
    "            print(f'No data for {name}, {start[0:4]}')\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b388a37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for name in names:\n",
    "    df_all = pd.DataFrame()\n",
    "    # Merge data from each year to one file\n",
    "    for start, end in zip(start_dates, end_dates):\n",
    "        filepath = Path(f'{pf}/data/output/bub_interval_estimates_nrows_{start[0:4]}-{end[0:4]}_VW{volume_weighted}_{name}.csv') # Set name \n",
    "        try:\n",
    "            df_results = pd.read_csv(filepath)\n",
    "            #print(filepath)\n",
    "        except FileNotFoundError:\n",
    "            #print(f'No data for {name}, {start[0:4]}')\n",
    "            continue\n",
    "        #df_results = pd.read_csv(filepath)\n",
    "        df_all = pd.concat((df_all, df_results), ignore_index=True)\n",
    "\n",
    "    # Save the results\n",
    "    filepath = Path(f'{pf}/data/output/bub_interval_estimates_final_{start_dates[0][0:4]}-{end_dates[-1][0:4]}_VW{volume_weighted}_{name}_all.csv') # Volume_weighted = False\n",
    "    df_all.to_csv(filepath, index=False)\n",
    "    #df_all.to_excel('testx.xlsx')\n",
    "    print(f'done {name}, {len(df_all)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfc36e3",
   "metadata": {},
   "source": [
    "# Bub plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a122d0-5077-4a4a-bd03-0eed545c795d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bub(name, start_dates, end_dates, volume_weighted, semax):\n",
    "    ma = 63\n",
    "    i = names.index(name)\n",
    "\n",
    "    # Read data\n",
    "    filepath= Path(f'{pf}/data/output/bub_interval_estimates_final_{start_dates[0][0:4]}-{end_dates[-1][0:4]}_VW{volume_weighted}_{name}_all.csv')\n",
    "    print(filepath)\n",
    "\n",
    "    df_main = pd.read_csv(filepath)\n",
    "\n",
    "    df_main['date'] =  pd.to_datetime(df_main['date'], format='%Y-%m-%d')\n",
    "    df_main = df_main.sort_values(['date', 'tau'], ascending = [True, True])\n",
    "\n",
    "    # Add maturity groups and bias corrected bubble estimate\n",
    "    df_main['tau_years'] = df_main['tau']/365 # tau in years\n",
    "    df_main['maturity_group'] = np.where(df_main['tau_years']<0.25, \"low\", np.where(df_main['tau_years']>0.5, \"high\", \"med\")) # group tau_years by, 0-0.25 = low, 0.25-0.50 = med, tau_years > 0.5 = high\n",
    "    #df_main['maturity_group'] = np.where(df_main['tau_years']<8/365, \"vlow\",  np.where((df_main['tau_years']<0.25)&(df_main['tau_years']<0.25), \"low\", np.where(df_main['tau_years']>0.5, \"high\", \"med\"))) #\n",
    "\n",
    "\n",
    "    # Smooth bubble estimates\n",
    "    print(len(df_main))\n",
    "\n",
    "    meancp = np.mean(df_main.sbub_qcdf)\n",
    "    meanp = np.mean(df_main.sbub_qcdfp)\n",
    "    meanc = np.mean(df_main.sbub_qcdfc)\n",
    "    print(len(df_main))\n",
    "\n",
    "    # Impute nonsensible values - replace with previous value if se > 1000\n",
    "    df_main['sbub_qcdf_se'] = np.real(df_main['sbub_qcdf_se'])\n",
    "    s = df_main['sbub_qcdf_se'] > semax # Find where se >1000\n",
    "    df_main.loc[s, \"sbub_qcdf_se\"] = np.nan # Set these equal to NAN\n",
    "    df_main[\"sbub_qcdf_se\"].ffill(inplace=True) # Fill NAN with previous valid obs\n",
    "\n",
    "    df_main['sbub_qcdfp_se'] = np.real(df_main['sbub_qcdf_se'])\n",
    "    s = df_main['sbub_qcdfp_se'] > semax # Find where se >1000\n",
    "    df_main.loc[s, \"sbub_qcdfp_se\"] = np.nan # Set these equal to NAN\n",
    "    df_main[\"sbub_qcdfp_se\"].ffill(inplace=True) # Fill NAN with previous valid obs\n",
    "\n",
    "    df_main['sbub_qcdfc_se'] = np.real(df_main['sbub_qcdf_se'])\n",
    "    s = df_main['sbub_qcdfc_se'] > semax # Find where se >1000\n",
    "    df_main.loc[s, \"sbub_qcdfc_se\"] = np.nan # Set these equal to NAN\n",
    "    df_main[\"sbub_qcdfc_se\"].ffill(inplace=True) # Fill NAN with previous valid obs\n",
    "    \n",
    "    df_main=df_main[df_main['qcdfc_bias']<1000]\n",
    "    df_main=df_main[df_main['qcdfp_bias']<1000]\n",
    "    df_main=df_main[df_main['qcdfc_bias']<1000]\n",
    "    df_main=df_main[df_main['qcdfc_bias']>-1000]\n",
    "    df_main=df_main[df_main['qcdfp_bias']>-1000]\n",
    "    df_main=df_main[df_main['qcdfc_bias']>-1000]\n",
    "\n",
    "    print(len(df_main))\n",
    "\n",
    "    # Setting up loop and variables\n",
    "    tau_grps = ['low', 'med', 'high', 'all']\n",
    "\n",
    "\n",
    "    period = df_main.date.unique()\n",
    "    nperiods = len(df_main.date.unique())\n",
    "    ntaugrps = len(tau_grps)\n",
    "    nem = 3 #number of estimation methods\n",
    "    mc = np.ceil(ma**0.25);\n",
    "\n",
    "    # for collecting results\n",
    "    bub_gp = np.zeros((nperiods, ntaugrps, nem))\n",
    "    sbubcdf_mu = np.zeros((nperiods, ntaugrps, nem)) ### Den her giver ikke mening, bare en tom en som de sætter ind i NW_COV, giver 0???\n",
    "    sbubcdf_se = np.zeros((nperiods, ntaugrps, nem))\n",
    "    sbubcdf_bc_mu = np.zeros((nperiods, ntaugrps, nem))\n",
    "    sbubcdf_bc_lb = np.zeros((nperiods, ntaugrps, nem))\n",
    "    sbubcdf_bc_ub = np.zeros((nperiods, ntaugrps, nem))\n",
    "    sbubcdf_bias = np.zeros((nperiods, ntaugrps, nem))\n",
    "\n",
    "    # Estimates by day and group\n",
    "    for t, date in enumerate(period):\n",
    "        for j, grp in enumerate(tau_grps):\n",
    "\n",
    "\n",
    "            # Subset date\n",
    "            df = df_main[df_main['date'] == date] # date\n",
    "\n",
    "            # Select group\n",
    "            if grp in ['low', 'med', 'high']: # if group is low, med or high, then select group otherwise include all\n",
    "                # Subset grp\n",
    "                df = df[df['maturity_group'] == grp] # group\n",
    "\n",
    "\n",
    "            # Bias corrected estimates\n",
    "            bub_gp[t, j, 0] = np.nanmean(df['sbub_qcdfp'] + df['qcdfp_bias']);\n",
    "            bub_gp[t, j, 1] = np.nanmean(df['sbub_qcdfc'] + df['qcdfc_bias']);\n",
    "            bub_gp[t, j, 2] = np.nanmean(df['sbub_qcdf'] + df['qcdf_bias']);\n",
    "            sbubcdf_bc_mu[t, j, 0] = bub_gp[t, j, 0] \n",
    "            sbubcdf_bc_mu[t, j, 1] = bub_gp[t, j, 1] \n",
    "            sbubcdf_bc_mu[t, j, 2] = bub_gp[t, j, 2] \n",
    "\n",
    "            # Standard error \n",
    "            ngp = len(df)\n",
    "            sbubcdf_se[t, j, 0] = np.sqrt(np.nansum(df['sbub_qcdfp_se']**2))/ngp;  \n",
    "            sbubcdf_se[t, j, 1] = np.sqrt(np.nansum(df['sbub_qcdfc_se']**2))/ngp;  \n",
    "            sbubcdf_se[t, j, 2] = np.sqrt(np.nansum(df['sbub_qcdf_se']**2))/ngp;  \n",
    "\n",
    "            # Bounds of A - used for confidence interval\n",
    "            sbubcdf_bc_lb[t, j, 0] = bub_gp[t, j, 0] - np.nanmean(df['qcdf_Ap_ub']);\n",
    "            sbubcdf_bc_ub[t, j, 0] = bub_gp[t, j, 0] - np.nanmean(df['qcdf_Ap_lb']);\n",
    "            sbubcdf_bc_lb[t, j, 1] = bub_gp[t, j, 1] - np.nanmean(df['qcdf_Ac_ub']);\n",
    "            sbubcdf_bc_ub[t, j, 1] = bub_gp[t, j, 1] - np.nanmean(df['qcdf_Ac_lb']);\n",
    "            sbubcdf_bc_lb[t, j, 2] = bub_gp[t, j, 2] - np.nanmean(df['qcdf_A_ub']);\n",
    "            sbubcdf_bc_ub[t, j, 2] = bub_gp[t, j, 2] - np.nanmean(df['qcdf_A_lb']);\n",
    "\n",
    "            #Bias\n",
    "            sbubcdf_bias[t, j, 0] = np.nanmean(df['qcdfp_bias']);\n",
    "            sbubcdf_bias[t, j, 1] = np.nanmean(df['qcdfc_bias']);\n",
    "            sbubcdf_bias[t, j, 2] = np.nanmean(df['qcdf_bias']); \n",
    "\n",
    "\n",
    "\n",
    "    # For collecting results\n",
    "    rsbubcdf_mu = np.zeros((nperiods, ntaugrps, nem))\n",
    "    rsbubcdf_se = np.zeros((nperiods, ntaugrps, nem))\n",
    "    rsbubcdf_bc_mu = np.zeros((nperiods, ntaugrps, nem))\n",
    "    rsbubcdf_bc_lb = np.zeros((nperiods, ntaugrps, nem))\n",
    "    rsbubcdf_bc_ub = np.zeros((nperiods, ntaugrps, nem))\n",
    "\n",
    "    # Moving averages:\n",
    "    for t in np.arange(ma, nperiods):\n",
    "        vt = np.arange(t-ma+1, t) # window of observations\n",
    "        for j, grp in enumerate(tau_grps):\n",
    "            for i in np.arange(0, nem): # estimation methods\n",
    "                #print(t, vt, j)\n",
    "\n",
    "                # Bubble moving average estimate\n",
    "                rsbubcdf_bc_mu[t,j,i] = np.nanmean(sbubcdf_bc_mu[vt,j,i]);\n",
    "\n",
    "                # Newey-West Standard error\n",
    "                rsbubcdf_mu[t,j,i] = np.nanmean(sbubcdf_mu[vt,j,i])\n",
    "                rsbubcdf_se[t,j,i] = np.sqrt((np.nansum(sbubcdf_se[vt,j,i]**2) + 2*nw_cov(sbubcdf_mu[vt,j,i],mc)))/ma\n",
    "\n",
    "                # Confidence interval\n",
    "                rsbubcdf_bc_lb[t,j,i] = np.nanmean(sbubcdf_bc_lb[vt,j,i]) - 1.96*rsbubcdf_se[t,j,i]\n",
    "                rsbubcdf_bc_ub[t,j,i] = np.nanmean(sbubcdf_bc_ub[vt,j,i]) + 1.96*rsbubcdf_se[t,j,i]\n",
    "\n",
    "    #print(f'mean bub est= {np.mean(df_main.sbub_qcdf)}, max bub est= {np.max(df_main.sbub_qcdf)}, min bub est= {np.min(df_main.sbub_qcdf)}')\n",
    "    #print(f'mean se est= {np.mean(df_main.sbub_qcdf_se)}, max se est= {np.max(df_main.sbub_qcdf_se)}, min se est= {np.min(df_main.sbub_qcdf_se)}')\n",
    "    #print(f'periods {nperiods}')\n",
    "    \n",
    "    return(rsbubcdf_bc_mu, rsbubcdf_bc_lb, rsbubcdf_bc_ub, period, bub_gp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4101fda0-342d-4a78-aff1-8cf6e667a611",
   "metadata": {},
   "source": [
    "## Stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d66fcc8-56d2-4349-b252-533b6ef62363",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "names = ['TSLA', 'AMZN', 'GOOGL']\n",
    "tsla = bub(names[0], start_dates, end_dates, volume_weighted, 1000)\n",
    "amzn = bub(names[1], start_dates, end_dates, volume_weighted, 1000)\n",
    "googl = bub(names[2], start_dates, end_dates, volume_weighted, 500)\n",
    "frames = [tsla, amzn, googl]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e3cd37d-cdef-4983-9593-f891d45e7b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6), constrained_layout=True)\n",
    "\n",
    "colors = ['blue', 'orange', 'green', 'red']\n",
    "start = start_dates[14]\n",
    "end = end_dates[-1]\n",
    "print(start, end)\n",
    "\n",
    "frame = frames[2]\n",
    "\n",
    "i = 2\n",
    "j = 0\n",
    "\n",
    "print(i,j)\n",
    "\n",
    "\n",
    "# plot moving average for bubble estimate\n",
    "ax.plot(tsla[3][:], tsla[0][:,j,i], label = f'TSLA', color=colors[0])\n",
    "ax.plot(amzn[3][:], amzn[0][:,j,i], label = f'AMZN', color=colors[1])\n",
    "ax.plot(googl[3][:], googl[0][:,j,i], label = f'GOOGL', color=colors[2])\n",
    "\n",
    "# plot confidence interval\n",
    "ax.fill_between(tsla[3][:], tsla[1][:,j,i], tsla[2][:,j,i], color=colors[0], alpha=.25)\n",
    "ax.fill_between(amzn[3][:], amzn[1][:,j,i], amzn[2][:,j,i], color=colors[1], alpha=.25)\n",
    "ax.fill_between(googl[3][:], googl[1][:,j,i], googl[2][:,j,i], color=colors[2], alpha=.25)\n",
    "\n",
    "# Axes, Labels, title etc \n",
    "ax.set_xlim([pd.to_datetime(start, format = '%Y-%m-%d'),pd.to_datetime(end, format = '%Y-%m-%d')])\n",
    "ax.set_ylabel(r'Bubble estimate')\n",
    "ax.set_title(r'$\\hat{\\Pi}_{cp}(\\tau)$', loc='center', fontsize='medium')\n",
    "ax.legend(loc='upper left')\n",
    "#ax.set_ylim(-60,200)\n",
    "\n",
    "filepath = Path(f'{pf}/data/figures/bubbles_stocks') # Set name\n",
    "plt.savefig(filepath) #bbox_inches='tight'\n",
    "    \n",
    "#ax.set_ylim(0, 100)\n",
    "#ax.set_ylim(-60, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80713bb-307f-4fc9-90ea-1b560aed7520",
   "metadata": {},
   "source": [
    "## Indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e4fce3-2e52-42a2-bafb-f47d55096ef8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "names = ['SPX', 'NDX', 'RUT']\n",
    "spx = bub(names[0], start_dates, end_dates, volume_weighted, 1000)\n",
    "ndx = bub(names[1], start_dates, end_dates, volume_weighted, 1000)\n",
    "rut = bub(names[2], start_dates, end_dates, volume_weighted, 1000)\n",
    "frames = [spx, ndx, rut]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969a7e61-5857-4408-9ea4-380d2f29f14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(10, 6), constrained_layout=True)\n",
    "colors = ['blue', 'red', 'orange', 'green']\n",
    "start = start_dates[0]\n",
    "end = end_dates[-1]\n",
    "print(start, end)\n",
    "\n",
    "frame = frames[2]\n",
    "\n",
    "i = 2\n",
    "j = 0\n",
    "\n",
    "print(i,j)\n",
    "\n",
    "\n",
    "# plot moving average for bubble estimate\n",
    "ax.plot(spx[3][:], spx[0][:,j,i], label = f'S&P 500', color=colors[0])\n",
    "ax.plot(ndx[3][:], ndx[0][:,j,i]/5, label = f'Nasdaq', color=colors[1])\n",
    "ax.plot(rut[3][:], rut[0][:,j,i], label = f'Russel 2000', color=colors[2])\n",
    "\n",
    "# plot confidence interval\n",
    "ax.fill_between(spx[3][:], spx[1][:,j,i], spx[2][:,j,i], color=colors[0], alpha=.25)\n",
    "ax.fill_between(ndx[3][:], ndx[1][:,j,i]/5, ndx[2][:,j,i]/5, color=colors[1], alpha=.25)\n",
    "ax.fill_between(rut[3][:], rut[1][:,j,i], rut[2][:,j,i], color=colors[2], alpha=.25)\n",
    "\n",
    "\n",
    "# Axes, Labels, title etc \n",
    "ax.set_xlim([pd.to_datetime(start, format = '%Y-%m-%d'),pd.to_datetime(end, format = '%Y-%m-%d')])\n",
    "ax.set_ylabel(r'Bubble estimate')\n",
    "ax.set_title(r'$\\hat{\\Pi}_{cp}(\\tau)$', loc='center', fontsize='medium')\n",
    "ax.legend(loc='upper left')\n",
    "#ax.set_ylim(-60,200)\n",
    "\n",
    "filepath = Path(f'{pf}/data/figures/bubbles_indices') # Set name\n",
    "plt.savefig(filepath) #bbox_inches='tight'\n",
    "    \n",
    "#ax.set_ylim(0, 100)\n",
    "ax.set_ylim(-60, 200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
